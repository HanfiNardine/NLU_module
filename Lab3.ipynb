{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFuj1We9_cuA",
        "outputId": "d969a631-921f-417e-e43b-4f3080bf1c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4WsMeVE-0Yu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zGfJw_7_8qU"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_S-ejMS-8zG"
      },
      "outputs": [],
      "source": [
        "\n",
        "# set up the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# load the data\n",
        "df = pd.read_csv('train.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyRCFaiK-8m5"
      },
      "outputs": [],
      "source": [
        "# split the data into prompt tuning set and evaluation benchmark\n",
        "train_size = int(0.1 * len(df))\n",
        "df_train = df[:train_size]\n",
        "df_val = df[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8LRGPRY-8ji"
      },
      "outputs": [],
      "source": [
        "# define the prompt engineering techniques to try\n",
        "prompt_techniques = [\n",
        "    lambda text: f\"disaster: {text}\",\n",
        "    lambda text: f\"{text} [SEP] disaster\",\n",
        "    lambda text: f\"disaster [SEP] {text}\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3mG40IR-8hd"
      },
      "outputs": [],
      "source": [
        "# set up hyperparameters\n",
        "epochs = 3\n",
        "batch_size = 16\n",
        "learning_rate = 2e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOzQiDQc-8fE",
        "outputId": "7cd489b2-b4bd-4f2b-831e-575f5ba92f96"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training model with prompt engineering technique 1\n",
            "\n",
            "Epoch 1/3\n",
            "Average training loss: 0.5199\n",
            "Accuracy: 0.7736\n",
            "Precision: 0.8388\n",
            "Recall: 0.5955\n",
            "F1-score: 0.6965\n",
            "\n",
            "Epoch 2/3\n",
            "Average training loss: 0.3722\n",
            "Accuracy: 0.7611\n",
            "Precision: 0.9344\n",
            "Recall: 0.4865\n",
            "F1-score: 0.6398\n",
            "\n",
            "Epoch 3/3\n",
            "Average training loss: 0.2849\n",
            "Accuracy: 0.7995\n",
            "Precision: 0.7961\n",
            "Recall: 0.7263\n",
            "F1-score: 0.7596\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training model with prompt engineering technique 2\n",
            "\n",
            "Epoch 1/3\n",
            "Average training loss: 0.5387\n",
            "Accuracy: 0.7512\n",
            "Precision: 0.9105\n",
            "Recall: 0.4764\n",
            "F1-score: 0.6255\n",
            "\n",
            "Epoch 2/3\n",
            "Average training loss: 0.3710\n",
            "Accuracy: 0.7882\n",
            "Precision: 0.8239\n",
            "Recall: 0.6544\n",
            "F1-score: 0.7294\n",
            "\n",
            "Epoch 3/3\n",
            "Average training loss: 0.2919\n",
            "Accuracy: 0.7808\n",
            "Precision: 0.7326\n",
            "Recall: 0.7835\n",
            "F1-score: 0.7572\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training model with prompt engineering technique 3\n",
            "\n",
            "Epoch 1/3\n",
            "Average training loss: 0.5673\n",
            "Accuracy: 0.7843\n",
            "Precision: 0.8662\n",
            "Recall: 0.5979\n",
            "F1-score: 0.7074\n",
            "\n",
            "Epoch 2/3\n",
            "Average training loss: 0.3985\n",
            "Accuracy: 0.7837\n",
            "Precision: 0.8167\n",
            "Recall: 0.6501\n",
            "F1-score: 0.7239\n",
            "\n",
            "Epoch 3/3\n",
            "Average training loss: 0.2713\n",
            "Accuracy: 0.7820\n",
            "Precision: 0.7524\n",
            "Recall: 0.7454\n",
            "F1-score: 0.7489\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# train and evaluate the model with each prompt engineering technique\n",
        "for i, prompt in enumerate(prompt_techniques):\n",
        "    # tokenize the data and add special tokens\n",
        "    train_texts = [prompt(text) for text in df_train['text'].tolist()]\n",
        "    val_texts = [prompt(text) for text in df_val['text'].tolist()]\n",
        "    train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "    val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "    # convert data to tensors\n",
        "    train_inputs = torch.tensor(train_encodings['input_ids'])\n",
        "    train_labels = torch.tensor(df_train['target'].tolist())\n",
        "    train_masks = torch.tensor(train_encodings['attention_mask'])\n",
        "\n",
        "    val_inputs = torch.tensor(val_encodings['input_ids'])\n",
        "    val_labels = torch.tensor(df_val['target'].tolist())\n",
        "    val_masks = torch.tensor(val_encodings['attention_mask'])\n",
        "\n",
        "    # set up the data loaders\n",
        "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "    val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "    val_sampler = SequentialSampler(val_data)\n",
        "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
        "\n",
        "    # set up the model\n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # train the model\n",
        "    print(f\"\\nTraining model with prompt engineering technique {i+1}\")\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_inputs, batch_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_inputs, token_type_ids=None, attention_mask=batch_masks, labels=batch_labels)\n",
        "            loss = outputs[0]\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
        "\n",
        "        # evaluate the model on the validation set\n",
        "        model.eval()\n",
        "        preds = []\n",
        "        true_labels = []\n",
        "        for batch in val_dataloader:\n",
        "            batch_inputs, batch_masks, batch_labels = tuple(t.to(device) for t in batch)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(batch_inputs, token_type_ids=None, attention_mask=batch_masks)\n",
        "\n",
        "            logits = outputs[0]\n",
        "            _, batch_pred = torch.max(logits, dim=1)\n",
        "            preds.extend(batch_pred.tolist())\n",
        "            true_labels.extend(batch_labels.tolist())\n",
        "\n",
        "        accuracy = accuracy_score(true_labels, preds)\n",
        "        precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, preds, average='binary')\n",
        "\n",
        "        print(f\"Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Precision: {precision:.4f}\")\n",
        "        print(f\"Recall: {recall:.4f}\")\n",
        "        print(f\"F1-score: {f1_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lQFyhK1V_qHX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}