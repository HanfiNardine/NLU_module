# -*- coding: utf-8 -*-
"""lab2-gov-qa-AITrio.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11xGKwz_du0tUktUhkhqzon2xnucu0hzD
"""

!pip install transformers

import requests
from bs4 import BeautifulSoup
import csv

# Step 1: Extract data using Beautiful Soup

url = "http://www.tunisie.gov.tn/"
r = requests.get(url)
soup = BeautifulSoup(r.content, 'html.parser')

# Extract all text data from the webpage
texts = []
for p in soup.find_all('p'):
    texts.append(p.get_text())

# Step 2: Store extracted data in CSV format

with open('tunisian_texts.csv', 'w', newline='') as file:
    writer = csv.writer(file)
    writer.writerow(["text"])
    for text in texts:
        writer.writerow([text])

from transformers import GPT2Tokenizer, GPT2LMHeadModel
import torch

# Step 3: Pre-train a GPT-2 language model on the extracted text data

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Load the text data from the CSV file
with open('tunisian_texts.csv', 'r') as file:
    reader = csv.reader(file)
    texts = [row[0] for row in reader]

# Tokenize the text data and convert to PyTorch tensors
input_ids = []
for text in texts:
    input_ids.extend(tokenizer.encode(text, add_special_tokens=True))

input_ids = torch.tensor(input_ids)

# Train the model for 10 epochs
model.train()
for epoch in range(10):
    loss = model(input_ids, labels=input_ids)[0]
    print(f"Epoch {epoch}: Loss {loss}")

# Step 4: Evaluate the model by question-answering

# Define a function to generate answers from the model
def generate_answer(model, tokenizer, question, max_length=50):
    # Encode the question
    input_ids = tokenizer.encode(question, add_special_tokens=True, return_tensors='pt')
    # Generate the answer
    output = model.generate(input_ids, max_length=max_length, do_sample=True)
    # Decode the answer and return it as a string
    return tokenizer.decode(output[0], skip_special_tokens=True)

# Example questions
questions = ["What is the capital of Tunisia?",
             "What is the official language of Tunisia?",
             "What is the currency of Tunisia?"]

# Generate answers using the pre-trained model
for question in questions:
    answer = generate_answer(model, tokenizer, question)
    print(f"Q: {question}")
    print(f"A: {answer}")